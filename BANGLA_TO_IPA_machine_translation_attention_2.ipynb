{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QROvg9mm2YwM"
      },
      "source": [
        "## The goal of this notebook is to introduce sequence to sequence language translation (seq2seq) and Attention mechanism.\n",
        "The notebook deals with a sequence to sequence model for English to Hindi translation. After training the model one will be able to input a English sentence and get back its Hindi translation.\n",
        "\n",
        ">RNNs are also capable of doing natural language translation, aka. machine translation. It involves two RNNs, one for the source language and one for the target language. One of them is called an encoder, and the other one decoder. The reason is that, the first one encodes the sentence into a vector and the second one converts the encoded vector into a sentence in target language. The decoder is a separete RNN. Given the encoded sentence, it produces the translated sentence in target language. Attention lets the decoder to focus on specific parts of the input sentence for each output word. This helps the input and output sentences to align with one another.\n",
        "\n",
        "We obtained the dataset used from Kaggle: https://www.kaggle.com/aiswaryaramachandran/hindienglish-corpora\n",
        "\n",
        "<h2> References: </h2>\n",
        "<li></a> Sequence to Sequence Learning with Neural Networks (Research Publication)</li>\n",
        "<li></a> https://www.tensorflow.org/tutorials/text/nmt_with_attention </li>\n",
        "<li></a> Using stochastic computation graphs formalism for optimization of sequence-to-sequence model (Research Publication) </li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GLiha6U2YwO"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0pxp45Ka2YwP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from string import digits\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import re\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "#tf.enable_eager_execution()\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "import unicodedata\n",
        "import io\n",
        "import time\n",
        "import warnings\n",
        "import sys\n",
        "\n",
        "\n",
        "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "#     for filename in filenames:\n",
        "#         print(os.path.join(dirname, filename))\n",
        "\n",
        "# PATH = \"../input/hindienglish-corpora/Hindi_English_Truncated_Corpus.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qTva58fJ3yTg"
      },
      "outputs": [],
      "source": [
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "PATH = \"/content/trainIPAdb_u.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "sRKjJFh14HLN",
        "outputId": "9fd3e369-6c40-4a6f-9a47-4f7807ca7fab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  \\\n",
              "0                     এরপরও তারা বকেয়া পরিশোধ করেনি।   \n",
              "1  আগে সুইস ব্যাংকে জমা টাকার কোনো প্রতিবেদন প্রক...   \n",
              "2                আদালত প্রতিষ্ঠানটি সিলগালা করে দেয়।   \n",
              "3  রায়ে তাদের দোষী সাব্যস্ত করা হলেও সাজা আগামী স...   \n",
              "4  গ্রেফতারের পর থেকে তাদের বাঁচাতে এলাকার প্রভাব...   \n",
              "5          দাম্পত্য জীবনে তাদের দুটি কন্যাসন্তান হয়।   \n",
              "6  তিনি এ ধরনের উদ্যোগ গ্রহণের জন্য আয়োজকদের ধন্য...   \n",
              "7  এলাকাবাসী জানান, যমুনা নদীতে ড্রেজার মেশিন দিয়...   \n",
              "8  চেক পোস্টে পৌঁছতেই ইস্রাফিলকে আটক করে তার কাগ...   \n",
              "9  তার চিৎকারে বাড়ির অন্যরা ছুটে এসে একটি জানালা ...   \n",
              "\n",
              "                                                 ipa  \n",
              "0              eɾpɔɾo t̪ɐɾɐ bɔkeʲɐ poɾɪʃod̪ʱ kɔɾenɪ।  \n",
              "1  ɐge suɪ̯s bɛŋke ɟɔmɐ tɐkɐɾ kono pɾot̪ɪbed̪ɔn p...  \n",
              "2         ɐd̪ɐlɔt̪ pɾot̪ɪʃtʱɐntɪ sɪlgɐlɐ koɾe d̪ee̯।  \n",
              "3  ɾɐe t̪ɐd̪eɾ d̪oʃɪ ʃɐbbost̪o kɔɾɐ holeo̯ ʃɐɟɐ ɐ...  \n",
              "4  gɾepʰt̪ɐɾeɾ pɔɾ t̪ʰeke t̪ɐd̪eɾ bɐ̃cɐt̪e elɐkɐɾ...  \n",
              "5  d̪ɐmpot̪t̪o ɟɪbone t̪ɐd̪eɾ d̪utɪ konnɐʃɔnt̪ɐn ...  \n",
              "6  t̪ɪnɪ e d̪ʱɔɾoneɾ ud̪d̪og gɾohoneɾ ɟonno ɐʲoɟk...  \n",
              "7  elɐkɐbɐʃɪ ɟɐnɐn, ɟomunɐ nod̪ɪt̪e dɾeɟɐɾ meʃɪn ...  \n",
              "8  cek poste pẽcʰt̪e͡ɪ̯ ɪsɾɐpʰɪlke ɐtok koɾe t̪ɐ...  \n",
              "9  t̪ɐɾ cɪt̪kɐɾe bɐɽɪɾ onnoɾɐ cʰute eʃe ektɪ ɟɐnɐ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0147ef24-38d0-41dc-a021-11f87d27ca84\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>ipa</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>এরপরও তারা বকেয়া পরিশোধ করেনি।</td>\n",
              "      <td>eɾpɔɾo t̪ɐɾɐ bɔkeʲɐ poɾɪʃod̪ʱ kɔɾenɪ।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>আগে সুইস ব্যাংকে জমা টাকার কোনো প্রতিবেদন প্রক...</td>\n",
              "      <td>ɐge suɪ̯s bɛŋke ɟɔmɐ tɐkɐɾ kono pɾot̪ɪbed̪ɔn p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>আদালত প্রতিষ্ঠানটি সিলগালা করে দেয়।</td>\n",
              "      <td>ɐd̪ɐlɔt̪ pɾot̪ɪʃtʱɐntɪ sɪlgɐlɐ koɾe d̪ee̯।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>রায়ে তাদের দোষী সাব্যস্ত করা হলেও সাজা আগামী স...</td>\n",
              "      <td>ɾɐe t̪ɐd̪eɾ d̪oʃɪ ʃɐbbost̪o kɔɾɐ holeo̯ ʃɐɟɐ ɐ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>গ্রেফতারের পর থেকে তাদের বাঁচাতে এলাকার প্রভাব...</td>\n",
              "      <td>gɾepʰt̪ɐɾeɾ pɔɾ t̪ʰeke t̪ɐd̪eɾ bɐ̃cɐt̪e elɐkɐɾ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>দাম্পত্য জীবনে তাদের দুটি কন্যাসন্তান হয়।</td>\n",
              "      <td>d̪ɐmpot̪t̪o ɟɪbone t̪ɐd̪eɾ d̪utɪ konnɐʃɔnt̪ɐn ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>তিনি এ ধরনের উদ্যোগ গ্রহণের জন্য আয়োজকদের ধন্য...</td>\n",
              "      <td>t̪ɪnɪ e d̪ʱɔɾoneɾ ud̪d̪og gɾohoneɾ ɟonno ɐʲoɟk...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>এলাকাবাসী জানান, যমুনা নদীতে ড্রেজার মেশিন দিয়...</td>\n",
              "      <td>elɐkɐbɐʃɪ ɟɐnɐn, ɟomunɐ nod̪ɪt̪e dɾeɟɐɾ meʃɪn ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>চেক পোস্টে পৌঁছতেই ইস্রাফিলকে আটক করে তার কাগ...</td>\n",
              "      <td>cek poste pẽcʰt̪e͡ɪ̯ ɪsɾɐpʰɪlke ɐtok koɾe t̪ɐ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>তার চিৎকারে বাড়ির অন্যরা ছুটে এসে একটি জানালা ...</td>\n",
              "      <td>t̪ɐɾ cɪt̪kɐɾe bɐɽɪɾ onnoɾɐ cʰute eʃe ektɪ ɟɐnɐ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0147ef24-38d0-41dc-a021-11f87d27ca84')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0147ef24-38d0-41dc-a021-11f87d27ca84 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0147ef24-38d0-41dc-a021-11f87d27ca84');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-bd60373d-f671-46c3-b590-311b1bd76320\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bd60373d-f671-46c3-b590-311b1bd76320')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-bd60373d-f671-46c3-b590-311b1bd76320 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 21999,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 21971,\n        \"samples\": [\n          \"\\u098f\\u09b0 \\u09aa\\u09b0 \\u09a7\\u09be\\u09b0\\u09be\\u09ac\\u09be\\u09b9\\u09bf\\u0995 \\u09a8\\u09bf\\u09b0\\u09cd\\u09ac\\u09be\\u099a\\u09a8\\u09c7\\u09b0 \\u09ae\\u09be\\u09a7\\u09cd\\u09af\\u09ae\\u09c7 \\u0987\\u09b0\\u09be\\u09a8\\u09c7\\u09b0 \\u09a4\\u09c7\\u09b9\\u09b0\\u09be\\u09a8\\u09c7 \\u0985\\u09a8\\u09c1\\u09b7\\u09cd\\u09a0\\u09c7\\u09df \\u0986\\u09a8\\u09cd\\u09a4\\u09b0\\u09cd\\u099c\\u09be\\u09a4\\u09bf\\u0995 \\u0987\\u09a8\\u09ab\\u09b0\\u09ae\\u09c7\\u099f\\u09bf\\u0995\\u09cd\\u09b8 \\u0985\\u09b2\\u09bf\\u09ae\\u09cd\\u09aa\\u09bf\\u09df\\u09be\\u09a1\\u09c7\\u09b0 \\u099c\\u09a8\\u09cd\\u09af \\u09ac\\u09be\\u0982\\u09b2\\u09be\\u09a6\\u09c7\\u09b6\\u09c7\\u09b0 \\u09b8\\u09a6\\u09b8\\u09cd\\u09af\\u09a6\\u09c7\\u09b0 \\u09a8\\u09bf\\u09b0\\u09cd\\u09ac\\u09be\\u099a\\u09a8 \\u0995\\u09b0\\u09be \\u09b9\\u09ac\\u09c7\\u0964\",\n          \"\\u09b6\\u09a8\\u09bf\\u09ac\\u09be\\u09b0 \\u09b8\\u0995\\u09be\\u09b2\\u09c7 \\u09b8\\u09bf\\u09b0\\u09be\\u099c\\u0997\\u099e\\u09cd\\u099c \\u09b8\\u09b0\\u0995\\u09be\\u09b0\\u09bf \\u0995\\u09b2\\u09c7\\u099c\\u09c7\\u09b0 \\u0986\\u09a8\\u09cd\\u09a4\\u0983\\u09ac\\u09bf\\u09ad\\u09be\\u0997 \\u09ab\\u09c1\\u099f\\u09ac\\u09b2 \\u0996\\u09c7\\u09b2\\u09be\\u09b0 \\u0989\\u09a6\\u09cd\\u09ac\\u09cb\\u09a7\\u09a8\\u09c0 \\u0985\\u09a8\\u09c1\\u09b7\\u09cd\\u09a0\\u09be\\u09a8\\u09c7 \\u09aa\\u09cd\\u09b0\\u09a7\\u09be\\u09a8 \\u0985\\u09a4\\u09bf\\u09a5\\u09bf \\u09b9\\u09bf\\u09b8\\u09c7\\u09ac\\u09c7 \\u09a4\\u09bf\\u09a8\\u09bf \\u098f\\u09b8\\u09ac \\u0995\\u09a5\\u09be \\u09ac\\u09b2\\u09c7\\u09a8\\u0964\",\n          \"\\u09af\\u09c1\\u0995\\u09cd\\u09a4\\u09b0\\u09be\\u099c\\u09cd\\u09af\\u09c7\\u09b0 \\u09b8\\u09cd\\u0995\\u099f\\u09b2\\u09cd\\u09af\\u09be\\u09a8\\u09cd\\u09a1\\u09c7\\u09b0 \\u0985\\u09cd\\u09af\\u09be\\u09ac\\u09be\\u09b0\\u09a1\\u09c7\\u09a8\\u09b6\\u09be\\u09df\\u09be\\u09b0\\u09c7 \\u099f\\u09cd\\u09b0\\u09be\\u09ae\\u09cd\\u09aa \\u0997\\u09b2\\u09ab \\u0987\\u09a8\\u09cd\\u099f\\u09be\\u09b0\\u09a8\\u09cd\\u09af\\u09be\\u09b6\\u09a8\\u09be\\u09b2\\u09c7\\u09b0 \\u0989\\u09a6\\u09cd\\u09ac\\u09cb\\u09a7\\u09a8\\u09c0\\u09a4\\u09c7 \\u0985\\u09a4\\u09bf\\u09a5\\u09bf\\u09a6\\u09c7\\u09b0 \\u09a6\\u09c7\\u09df\\u09be \\u09b9\\u09df \\u098f \\u09b9\\u09c1\\u0987\\u09b8\\u09cd\\u0995\\u09bf\\u0964\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ipa\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 21967,\n        \"samples\": [\n          \"m\\u0250nu\\u0283t\\u026a \\u025bkb\\u0250\\u027e\\u026a mo\\u027ee,\\u09b6\\u09b0\\u09c0\\u09b0\\u09c0 m\\u027e\\u026at\\u032at\\u032au\\u027e \\u0250ge kob\\u026a\\u027e\\u0250 m\\u0254none m\\u0250ne h\\u0250\\u025f\\u0250\\u027e m\\u0254\\u027eon\\u0964 \",\n          \"\\u025f\\u0250n\\u0250 \\u025f\\u0250e\\u032f, \\u0250tokk\\u027e\\u026at\\u032ao \\u026a\\u02b2\\u0250b\\u0250 \\u0283\\u0254m\\u027e\\u0250t \\u0250lomog\\u026a\\u027e d\\u032a\\u026a\\u027eg\\u02b1od\\u032a\\u026an d\\u032a\\u02b1o\\u027ee pul\\u026a\\u0283ke p\\u02b0\\u0250\\u0303k\\u026a d\\u032a\\u026a\\u02b2e b\\u026ab\\u02b1\\u026anno el\\u0250k\\u0250e\\u032f moto\\u027es\\u0250\\u026akel\\u025foge \\u026a\\u02b2\\u0250b\\u0250 b\\u025bb\\u0283\\u0250 ko\\u027ee \\u0250\\u0283c\\u02b0e\\u0964 \",\n          \"\\u0250d\\u032a\\u02b1\\u026apot\\u032at\\u032ao b\\u026ast\\u032a\\u0250\\u027e o pu\\u027ebo\\u0283ot\\u032a\\u027eo\\u201ct\\u032a\\u0250\\u027e \\u025fe\\u027e d\\u032a\\u02b1o\\u027ee d\\u032a\\u026a\\u027eg\\u02b1od\\u032a\\u026an \\u027eo\\u0283ulpu\\u027e g\\u027e\\u0250me\\u027e nu\\u027e\\u0250\\u027eb\\u0250\\u027d\\u026a o k\\u026ap\\u026a\\u014bb\\u0250\\u027d\\u026a\\u027e b\\u0254\\u014b\\u0283e\\u027e lok\\u025f\\u0254n \\u025bk\\u0250d\\u032a\\u02b1\\u026akb\\u0250\\u027e \\u0283\\u0254\\u014bg\\u02b1\\u0254\\u027e\\u0283e l\\u026apt\\u032ao h\\u0254e\\u032f\\u0964\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the file path\n",
        "file_path = '/content/trainIPAdb_u.csv'\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv(file_path, encoding='utf-8')\n",
        "\n",
        "\n",
        "df.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_F8itiV2YwQ"
      },
      "source": [
        "## Preprocess English and Hindi sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PEvxPshq2YwQ"
      },
      "outputs": [],
      "source": [
        "# def unicode_to_ascii(s):\n",
        "#     return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "#         if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "# def preprocess_sentence(w):\n",
        "#     w = unicode_to_ascii(w.lower().strip())\n",
        "#     w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "#     w = re.sub(r'[\" \"]+', \" \", w)\n",
        "#     w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "#     w = w.rstrip().strip()\n",
        "#     return w\n",
        "\n",
        "# def hindi_preprocess_sentence(w):\n",
        "#     w = w.rstrip().strip()\n",
        "#     return w"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import re\n",
        "\n",
        "# def preprocess_sentence(w):\n",
        "#     # Normalizing the Unicode string so that similar characters are represented in the same form\n",
        "#     w = re.sub(r\"[\\u0980-\\u09FF]+\", \" \", w)\n",
        "\n",
        "#     # Tokenize the punctuation. Note: Bengali punctuation marks like '।' (daari) need to be considered\n",
        "#     w = re.sub(r\"([?.!,¿।])\", r\" \\1 \", w)\n",
        "\n",
        "#     # Replace multiple spaces with a single space\n",
        "#     w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "#     # Removing characters that are not part of Bengali Unicode block, space, or punctuation\n",
        "#     w = re.sub(r\"[^\\u0980-\\u09FF\\s?.!,¿।]+\", \" \", w)\n",
        "\n",
        "#     # Stripping whitespace from the beginning and the end of the sentence\n",
        "#     w = w.rstrip().strip()\n",
        "\n",
        "#     return w\n",
        "\n",
        "# # Example usage with a Bengali sentence\n",
        "# sample_text = \"এটি পরীক্ষার একটি বাক্য।\"\n",
        "# print(preprocess_sentence_bn(sample_text))\n",
        "\n",
        "# def hindi_preprocess_sentence(w):\n",
        "#     w = w.rstrip().strip()\n",
        "#     return w"
      ],
      "metadata": {
        "id": "ZpL9Mf7kLxsB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def unicode_to_ascii(s):\n",
        "    return s.encode('ascii', 'ignore').decode('ascii')\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    # This step is not necessary for Bangla since we do not convert it to ASCII.\n",
        "    # Presumably, the original function's intention was to normalize unicode characters\n",
        "    # to their closest ASCII representation which is not applicable here.\n",
        "    # w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "    w = w.strip().lower()\n",
        "\n",
        "    # Replace standard punctuation with spaces + punctuation. If Bangla has other\n",
        "    # punctuation marks, they should be included in the regular expression.\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "\n",
        "    # Replace multiple spaces with a single space\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "    # Use the Unicode range for Bangla to retain only the Bangla script characters\n",
        "    # and standard punctuation. This regex might need to be adjusted if there are\n",
        "    # additional Bangla-specific punctuation marks to include.\n",
        "    w = re.sub(r\"[^\\u0980-\\u09FF?.!,¿]+\", \" \", w)\n",
        "\n",
        "    w = w.rstrip().strip()\n",
        "\n",
        "    return w\n",
        "\n",
        "def hindi_preprocess_sentence(w):\n",
        "    w = w.rstrip().strip()\n",
        "    return w\n",
        "# # Example usage:\n",
        "# sentence = \"আমি বাংলায় গান গাই।\"\n",
        "# print(preprocess_sentence_bangla(sentence))"
      ],
      "metadata": {
        "id": "Fe0Na4l7KeFA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yJTQScjn2YwR"
      },
      "outputs": [],
      "source": [
        "def create_dataset(path=PATH):\n",
        "    lines=pd.read_csv(path,encoding='utf-8')\n",
        "    lines=lines.dropna()\n",
        "    #lines = lines[lines['source']=='ted']\n",
        "    en = []\n",
        "    hd = []\n",
        "    for i, j in zip(lines['text'], lines['ipa']):\n",
        "        en_1 = [preprocess_sentence(w) for w in i.split(' ')]\n",
        "        en_1.append('<end>')\n",
        "        en_1.insert(0, '<start>')\n",
        "        hd_1 = [hindi_preprocess_sentence(w) for w in j.split(' ')]\n",
        "        hd_1.append('<end>')\n",
        "        hd_1.insert(0, '<start>')\n",
        "        en.append(en_1)\n",
        "        hd.append(hd_1)\n",
        "    return hd, en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qe_Fzad62YwR"
      },
      "outputs": [],
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFQomB-G2YwR"
      },
      "source": [
        "### Tokenization of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PJNzwcuB2YwS"
      },
      "outputs": [],
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
        "  return tensor, lang_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LINKv71e2YwS"
      },
      "outputs": [],
      "source": [
        "def load_dataset(path=PATH):\n",
        "    targ_lang, inp_lang = create_dataset(path)\n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vNfoexCR2YwS"
      },
      "outputs": [],
      "source": [
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(PATH)\n",
        "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymO3KirM2YwS"
      },
      "source": [
        "### Create Train and Test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "fisPQsbt2YwS",
        "outputId": "2f23f407-09ea-437d-c570-76fe6d2a3259",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17599 17599 4400 4400\n"
          ]
        }
      ],
      "source": [
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mJ9BeHS2YwT",
        "outputId": "8ba634cb-a7d0-4752-c057-f6ee8fa39b11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "43 ----> পরে\n",
            "111 ----> স্থানীয়\n",
            "206 ----> লোকজন\n",
            "208 ----> এসে\n",
            "941 ----> আহতদের\n",
            "76 ----> উদ্ধার\n",
            "5 ----> করে\n",
            "67 ----> উপজেলা\n",
            "321 ----> স্বাস্থ্য\n",
            "589 ----> কমপ্লেক্সে\n",
            "171 ----> ভর্তি\n",
            "5 ----> করে\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "45 ----> pɔɾe\n",
            "108 ----> st̪ʰɐnɪʲo\n",
            "220 ----> lokɟɔn\n",
            "205 ----> eʃe\n",
            "900 ----> ɐht̪ɔd̪eɾ\n",
            "73 ----> ud̪d̪ʱɐɾ\n",
            "6 ----> koɾe\n",
            "62 ----> upoɟelɐ\n",
            "316 ----> ʃɐst̪ʰo\n",
            "576 ----> kɔmplekse\n",
            "165 ----> bʱɔɾt̪ɪ\n",
            "34 ----> koɾe।\n",
            "2 ----> <end>\n"
          ]
        }
      ],
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
        "\n",
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDyKvoZF2YwT"
      },
      "source": [
        "### Create Dataset\n",
        "> We are using minimal configuration as the notebbok is not focussed on metrics performance but rather the implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "iSFNAH6H2YwT"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 128\n",
        "units = 256\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtEU_yAZ2YwT"
      },
      "source": [
        "## Encoder Decoder with Attention Model\n",
        "\n",
        "> Encoder Decoder with Attention model is a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. It uses a multilayered Gated Recurrent Unit (GRU) to map the input sequence to a vector of a fixed dimensionality, and then another deep GRU to decode the target sequence from the vector.\n",
        "<img src=\"https://www.researchgate.net/profile/Vlad_Zhukov2/publication/321210603/figure/fig1/AS:642862530191361@1530281779831/An-example-of-sequence-to-sequence-model-with-attention-Calculation-of-cross-entropy.png\" width=\"800\" alt=\"attention mechanism\">\n",
        "\n",
        "> A sequence to sequence model has two parts – an encoder and a decoder. Both the parts are practically two different neural network models combined into one giant network. the task of an encoder network is to understand the input sequence, and create a smaller dimensional representation of it. This representation is then forwarded to a decoder network which generates a sequence of its own that represents the output. The input is put through an encoder model which gives us the encoder output. Here, each input words is assigned a weight by the attention mechanism which is then used by the decoder to predict the next word in the sentence. We use Bahdanau attention for the encoder.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehUetCri2YwT"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Vog9yoIg2YwT"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))\n",
        "\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uBVHYAu2YwT"
      },
      "source": [
        "### Attention Mechanism"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "vQMiHo3O2YwU"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "    return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkamfNvL2YwU"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xp7PqsH42YwU"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "    x = self.embedding(x)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "    output, state = self.gru(x)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    x = self.fc(output)\n",
        "    return x, state, attention_weights\n",
        "\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idsZq4gW2YwU"
      },
      "source": [
        "### Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "kyt39KOk2YwU"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "#   print(type(mask))\n",
        "  loss_ *= mask\n",
        "  return tf.reduce_mean(loss_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Zeb5PW2I2YwU"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnUEiQ9k2YwU"
      },
      "source": [
        "## Training\n",
        "\n",
        ">1. Pass *input* through *encoder* to get *encoder output*..\n",
        ">2. Then encoder output, encoder hidden state and the decoder input is passed to decoder.\n",
        ">3. Decoder returns *predictions* and *decoder hidden state*.\n",
        ">4. Decoder hidden state is then passed back to model.\n",
        ">5. Predictions are used to calculate loss.\n",
        ">6. Use *teacher forcing* (technique where the target word is passed as the next input to the decoder) for the next input to the decoder.\n",
        ">7. Calculate gradients and apply it to *optimizer* for backpropogation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "xDiu4nXt2YwU"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "    # Teacher forcing\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "  return batch_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODLE"
      ],
      "metadata": {
        "id": "3L-332RvPEvU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ir2s6nPD2YwU",
        "outputId": "e96035e1-24a2-46ba-9c2f-114955b1927e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 5.1819\n",
            "Epoch 1 Batch 100 Loss 4.5429\n",
            "Epoch 1 Batch 200 Loss 4.3472\n",
            "Epoch 1 Loss 4.3404\n",
            "Time taken for 1 epoch 66.34767842292786 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 4.0246\n",
            "Epoch 2 Batch 100 Loss 3.9898\n",
            "Epoch 2 Batch 200 Loss 3.8891\n",
            "Epoch 2 Loss 4.0334\n",
            "Time taken for 1 epoch 22.30234169960022 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 4.0266\n",
            "Epoch 3 Batch 100 Loss 3.6104\n",
            "Epoch 3 Batch 200 Loss 4.1237\n",
            "Epoch 3 Loss 3.8691\n",
            "Time taken for 1 epoch 20.041576147079468 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 3.7703\n",
            "Epoch 4 Batch 100 Loss 3.8170\n",
            "Epoch 4 Batch 200 Loss 3.9453\n",
            "Epoch 4 Loss 3.6812\n",
            "Time taken for 1 epoch 20.51960062980652 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 3.5606\n",
            "Epoch 5 Batch 100 Loss 3.3452\n",
            "Epoch 5 Batch 200 Loss 3.4956\n",
            "Epoch 5 Loss 3.3108\n",
            "Time taken for 1 epoch 19.719292402267456 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 2.5613\n",
            "Epoch 6 Batch 100 Loss 3.0453\n",
            "Epoch 6 Batch 200 Loss 3.1355\n",
            "Epoch 6 Loss 3.0826\n",
            "Time taken for 1 epoch 25.316540718078613 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 2.5166\n",
            "Epoch 7 Batch 100 Loss 2.7476\n",
            "Epoch 7 Batch 200 Loss 2.3827\n",
            "Epoch 7 Loss 2.5425\n",
            "Time taken for 1 epoch 19.39306139945984 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 2.4864\n",
            "Epoch 8 Batch 100 Loss 2.2846\n",
            "Epoch 8 Batch 200 Loss 2.0999\n",
            "Epoch 8 Loss 2.2723\n",
            "Time taken for 1 epoch 20.08431124687195 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 2.4916\n",
            "Epoch 9 Batch 100 Loss 1.7473\n",
            "Epoch 9 Batch 200 Loss 2.1759\n",
            "Epoch 9 Loss 2.1128\n",
            "Time taken for 1 epoch 19.943310260772705 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 2.1231\n",
            "Epoch 10 Batch 100 Loss 1.6033\n",
            "Epoch 10 Batch 200 Loss 2.0372\n",
            "Epoch 10 Loss 1.9519\n",
            "Time taken for 1 epoch 24.997583389282227 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 2.4356\n",
            "Epoch 11 Batch 100 Loss 3.7428\n",
            "Epoch 11 Batch 200 Loss 3.9471\n",
            "Epoch 11 Loss 3.4700\n",
            "Time taken for 1 epoch 18.99983787536621 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 3.4219\n",
            "Epoch 12 Batch 100 Loss 3.2857\n",
            "Epoch 12 Batch 200 Loss 2.6204\n",
            "Epoch 12 Loss 2.9554\n",
            "Time taken for 1 epoch 20.3464298248291 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 2.0014\n",
            "Epoch 13 Batch 100 Loss 1.9679\n",
            "Epoch 13 Batch 200 Loss 1.9611\n",
            "Epoch 13 Loss 2.1789\n",
            "Time taken for 1 epoch 19.550344228744507 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 1.6149\n",
            "Epoch 14 Batch 100 Loss 2.1205\n",
            "Epoch 14 Batch 200 Loss 1.8501\n",
            "Epoch 14 Loss 1.9767\n",
            "Time taken for 1 epoch 25.223756313323975 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 1.4769\n",
            "Epoch 15 Batch 100 Loss 1.7236\n",
            "Epoch 15 Batch 200 Loss 1.5090\n",
            "Epoch 15 Loss 1.6341\n",
            "Time taken for 1 epoch 19.41978359222412 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 1.3387\n",
            "Epoch 16 Batch 100 Loss 1.3632\n",
            "Epoch 16 Batch 200 Loss 1.5253\n",
            "Epoch 16 Loss 1.3780\n",
            "Time taken for 1 epoch 25.1728458404541 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 1.0697\n",
            "Epoch 17 Batch 100 Loss 1.1617\n",
            "Epoch 17 Batch 200 Loss 1.2091\n",
            "Epoch 17 Loss 1.1981\n",
            "Time taken for 1 epoch 19.42468237876892 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 1.1374\n",
            "Epoch 18 Batch 100 Loss 0.9597\n",
            "Epoch 18 Batch 200 Loss 1.0721\n",
            "Epoch 18 Loss 1.1254\n",
            "Time taken for 1 epoch 20.18457293510437 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 1.0688\n",
            "Epoch 19 Batch 100 Loss 1.0517\n",
            "Epoch 19 Batch 200 Loss 0.8819\n",
            "Epoch 19 Loss 0.9765\n",
            "Time taken for 1 epoch 19.19089698791504 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.9163\n",
            "Epoch 20 Batch 100 Loss 0.8223\n",
            "Epoch 20 Batch 200 Loss 0.8859\n",
            "Epoch 20 Loss 0.9001\n",
            "Time taken for 1 epoch 25.047111988067627 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.7910\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 100\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "    if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                     batch,\n",
        "                                                     batch_loss.numpy()))\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euTZN8-q2YwV"
      },
      "outputs": [],
      "source": [
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                           maxlen=max_length_inp,\n",
        "                                                           padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    result = ''\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        result += targ_lang.index_word[predicted_id] + ' '\n",
        "        if targ_lang.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    return result, sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DC0eMlbT2YwV"
      },
      "outputs": [],
      "source": [
        "def translate(sentence):\n",
        "    result, sentence = evaluate(sentence)\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVxbt5w52YwV"
      },
      "outputs": [],
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(20)"
      ],
      "metadata": {
        "id": "bqgr_p4yRM0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aU1fDujI2YwV"
      },
      "outputs": [],
      "source": [
        "# আদালত প্রতিষ্ঠানটি সিলগালা করে দেয়।\n",
        "# ɐd̪ɐlɔt̪ pɾot̪ɪʃtʱɐntɪ sɪlgɐlɐ koɾe d̪ee̯\n",
        "#প্রকল্প বাস্তবায়ন হলে আধুনিক শিক্ষার সঙ্গে\n",
        "#pɾokɔlpo bɐst̪obɐʲon hole ɐd̪ʱunɪk ʃɪkkʰɐɾ\n",
        "\n",
        "translate(u'এখন কী হবে- জানতে চাইলে')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RESULT"
      ],
      "metadata": {
        "id": "S6ytUArsPciH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# আদালত প্রতিষ্ঠানটি সিলগালা করে দেয়।\n",
        "# ɐd̪ɐlɔt̪ pɾot̪ɪʃtʱɐntɪ sɪlgɐlɐ koɾe d̪ee̯\n",
        "\n",
        "# E-10\n",
        "# pulɪʃ cɐɪ̯le hɔlen- t̪ɐɾ bɪɾud̪d̪ʱe ɐtok hɔe̯।\n",
        "# E-25\n",
        "# ɐd̪ɐlɔt̪ pɾot̪ɪʃtʱɐntɪ sɪlgɐlɐ koɾe d̪ee̯।\n",
        "\n"
      ],
      "metadata": {
        "id": "pjXf3e1jPe6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}